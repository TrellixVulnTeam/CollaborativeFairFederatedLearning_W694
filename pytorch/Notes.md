
## Adult dataset
### Overall default settings:

	- lr = 0.1
	- grad_clip = 0.01
	- B = 16
	- E = 5
	- pretrain = 5
	- fl_epochs = 100
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- n_freerider = 1 -> one free-rider with random weights (uniformly generated gradients in [0, 1])
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'sum'
	- lr decay gamma : 0.955


## Settings for experiment 1:
	- lr = 0.1
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'sum'
	- lr decay gamma : 0.955
	- NOTE: To show that lr decay helps with both training stability -> acc and high fairness

### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |    -0.0339223 | 0.740149 |        0.836696 |
	| P10_1.0 |     0.130622  | 0.790421 |        0.875604 |
	| P20_0.1 |     0.18718   | 0.528966 |        0.849942 |
	| P20_1.0 |     0.194114  | 0.5962   |        0.860691 |
	| P5_0.1  |     0.477631  | 0.606913 |        0.966291 |
	| P5_1.0  |     0.142717  | 0.593769 |        0.924197 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.807939 |  0.808341 |  0.794603 |  0.801115 | 0.804505 | 0.805397 |
	| Standalone    |  0.768198 |  0.77074  |  0.757716 |  0.75901  | 0.777475 | 0.780598 |
	| CFFL          |  0.796432 |  0.796521 |  0.801918 |  0.801695 | 0.794826 | 0.797904 |
	| CFFL pretrain |  0.78595  |  0.790232 |  0.780464 |  0.783675 | 0.789652 | 0.792685 |



## Settings for experiment 2:
	- lr = 0.001
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'mean' ->fedavg
	- no lr decay
	- NOTE: To show that fedavg helps with both training stability -> acc and good fairness


### Results
	|         |   Distriubted |      CFFL |   CFFL pretrain |
	|:--------|--------------:|----------:|----------------:|
	| P10_0.1 |     0.17599   |  0.118817 |        0.606401 |
	| P10_1.0 |    -0.08173   |  0.171829 |        0.639351 |
	| P20_0.1 |     0.424413  |  0.631835 |        0.802583 |
	| P20_1.0 |     0.425838  |  0.430256 |        0.738453 |
	| P5_0.1  |    -0.139916  | -0.156414 |        0.686485 |
	| P5_1.0  |     0.0647306 | -0.35913  |        0.632759 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.806557 |  0.805308 |  0.794068 |  0.796476 | 0.803479 | 0.805754 |
	| Standalone    |  0.762801 |  0.75504  |  0.774175 |  0.781713 | 0.771097 | 0.768332 |
	| CFFL          |  0.793176 |  0.789161 |  0.789607 |  0.792061 | 0.796655 | 0.797547 |
	| CFFL pretrain |  0.788359 |  0.785192 |  0.792239 |  0.792596 | 0.788938 | 0.797413 |
_Note_: using 'fedavg' without lr decay, the fairness is ok, and the acc is good, too.


## Settings for experiment 3:
	- lr = 0.1
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- n_freerider = 1
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'sum'
	- lr decay gamma: 0.955
	- theta [0.1, 0.2, 0.3, ..., 1]
	- NOTE: To study that how thetas affect the ability to detect the free rider

### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |    0.105185   | 0.986006 |        0.519266 |
	| P10_0.2 |   -0.0655986  | 0.983765 |        0.551001 |
	| P10_0.3 |    0.16788    | 0.983463 |        0.213435 |
	| P10_0.4 |    0.135792   | 0.988212 |        0.307276 |
	| P10_0.5 |    0.0439631  | 0.987719 |        0.259561 |
	| P10_0.6 |    0.195997   | 0.977675 |        0.28809  |
	| P10_0.7 |    0.19194    | 0.987601 |        0.291511 |
	| P10_0.8 |    0.0346704  | 0.973317 |        0.37468  |
	| P10_0.9 |   -0.0942054  | 0.987359 |        0.32411  |
	| P10_1.0 |    0.0953071  | 0.984069 |        0.269798 |
	| P20_0.1 |    0.12298    | 0.958344 |        0.533477 |
	| P20_0.2 |    0.136276   | 0.971284 |        0.690441 |
	| P20_0.3 |    0.128131   | 0.967549 |        0.51882  |
	| P20_0.4 |    0.0434694  | 0.969155 |        0.559367 |
	| P20_0.5 |    0.115418   | 0.968247 |        0.582427 |
	| P20_0.6 |    0.150685   | 0.968124 |        0.740632 |
	| P20_0.7 |    0.11669    | 0.957656 |        0.675908 |
	| P20_0.8 |    0.185185   | 0.964138 |        0.707295 |
	| P20_0.9 |    0.197086   | 0.9711   |        0.604744 |
	| P20_1.0 |    0.0616851  | 0.969999 |        0.954794 |
	| P5_0.1  |    0.124092   | 0.991453 |        0.911732 |
	| P5_0.2  |   -0.0375307  | 0.993616 |        0.883985 |
	| P5_0.3  |   -0.111941   | 0.993857 |        0.810668 |
	| P5_0.4  |   -0.00544177 | 0.991553 |        0.892398 |
	| P5_0.5  |    0.0782737  | 0.990098 |        0.748683 |
	| P5_0.6  |    0.121944   | 0.987513 |        0.803578 |
	| P5_0.7  |   -0.0295329  | 0.991705 |        0.670817 |
	| P5_0.8  |   -0.0438124  | 0.991161 |        0.73763  |
	| P5_0.9  |    0.131708   | 0.989338 |        0.838311 |
	| P5_1.0  |    0.0498815  | 0.991849 |        0.803708 |
	|               |   P10_0.1 |   P10_0.2 |   P10_0.3 |   P10_0.4 |   P10_0.5 |   P10_0.6 |   P10_0.7 |   P10_0.8 |   P10_0.9 |   P10_1.0 |   P20_0.1 |   P20_0.2 |   P20_0.3 |   P20_0.4 |   P20_0.5 |   P20_0.6 |   P20_0.7 |   P20_0.8 |   P20_0.9 |   P20_1.0 |   P5_0.1 |   P5_0.2 |   P5_0.3 |   P5_0.4 |   P5_0.5 |   P5_0.6 |   P5_0.7 |   P5_0.8 |   P5_0.9 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|
	| Distributed   |  0.808653 |  0.809367 |  0.806958 |  0.810303 |  0.808564 |  0.807003 |  0.806378 |  0.807939 |  0.807315 |  0.80562  |  0.797591 |  0.79893  |  0.797235 |  0.805352 |  0.800312 |  0.795094 |  0.805888 |  0.79273  |  0.795896 |  0.803524 | 0.805977 | 0.804996 | 0.802542 | 0.805888 | 0.80562  | 0.803836 | 0.804505 | 0.807359 | 0.807359 | 0.806066 |
	| Standalone    |  0.768778 |  0.773773 |  0.769358 |  0.765254 |  0.767217 |  0.766994 |  0.76909  |  0.769224 |  0.767618 |  0.769625 |  0.767529 |  0.771632 |  0.767351 |  0.766057 |  0.770874 |  0.76463  |  0.769045 |  0.767618 |  0.768376 |  0.763158 | 0.779483 | 0.780018 | 0.776182 | 0.776137 | 0.775736 | 0.781579 | 0.779572 | 0.777832 | 0.780107 | 0.778724 |
	| CFFL          |  0.728412 |  0.734434 |  0.740098 |  0.738002 |  0.742016 |  0.742864 |  0.741659 |  0.731044 |  0.730508 |  0.743176 |  0.748171 |  0.749286 |  0.754014 |  0.753122 |  0.750401 |  0.75116  |  0.749688 |  0.754996 |  0.750803 |  0.746566 | 0.730776 | 0.718064 | 0.724532 | 0.719625 | 0.722034 | 0.727163 | 0.726628 | 0.726851 | 0.719402 | 0.723595 |
	| CFFL pretrain |  0.786084 |  0.789607 |  0.788359 |  0.777119 |  0.786084 |  0.788938 |  0.787957 |  0.789161 |  0.786753 |  0.790856 |  0.781178 |  0.791079 |  0.783631 |  0.778234 |  0.791258 |  0.782649 |  0.781311 |  0.783854 |  0.78537  |  0.783274 | 0.793756 | 0.790901 | 0.791302 | 0.790678 | 0.790856 | 0.793176 | 0.793176 | 0.795049 | 0.793443 | 0.792863 |

## Settings for experiment 4:
	- lr = 0.001
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- n_freerider = 1
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'mean' ->fedavg
	- no lr decay
	- NOTE: To study if fedavg can affect the ability to detect the free rider


### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |     0.0752315 | 0.988631 |        0.635469 |
	| P10_1.0 |     0.151269  | 0.982865 |        0.258348 |
	| P20_0.1 |     0.164302  | 0.983251 |        0.54308  |
	| P20_1.0 |     0.0722481 | 0.982241 |        0.391062 |
	| P5_0.1  |    -0.117507  | 0.994388 |        0.842333 |
	| P5_1.0  |     0.0779394 | 0.983099 |        0.569457 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.809233 |  0.806913 |  0.798573 |  0.801249 | 0.804728 | 0.806423 |
	| Standalone    |  0.763872 |  0.73479  |  0.766949 |  0.767038 | 0.778591 | 0.782114 |
	| CFFL          |  0.763292 |  0.768599 |  0.769447 |  0.772748 | 0.775067 | 0.775781 |
	| CFFL pretrain |  0.757939 |  0.743265 |  0.767306 |  0.764987 | 0.766949 | 0.768287 |
	[Finished in 6.1s]
_Notes_ : using 'fedavg' only without decay still __cannot__ effectively take out free rider.