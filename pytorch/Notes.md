
## Adult dataset
### Overall default settings:

	- lr = 0.1
	- grad_clip = 0.01
	- B = 16
	- E = 5
	- pretrain = 5
	- fl_epochs = 100
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- n_freerider = 1 -> one free-rider with random weights (uniformly generated gradients in [0, 1])
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'sum'
	- lr decay gamma : 0.955


## Settings for experiment 1:
	- lr = 0.1
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'sum'
	- lr decay gamma : 0.977 -> 0.977^19 ~= 0.1
	- NOTE: To show that lr decay helps with both training stability -> acc and high fairness
	- folder name: Experiments_2020-05-01-14:23

### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |     0.0781112 | 0.786077 |        0.911945 |
	| P10_1.0 |     0.121521  | 0.819093 |        0.913104 |
	| P20_0.1 |     0.139235  | 0.671063 |        0.885361 |
	| P20_1.0 |     0.240523  | 0.768404 |        0.88128  |
	| P5_0.1  |     0.0497319 | 0.589754 |        0.927291 |
	| P5_1.0  |     0.14527   | 0.670131 |        0.952378 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.805754 |  0.808341 |  0.814139 |  0.81182  | 0.805219 | 0.804416 |
	| Standalone    |  0.769581 |  0.77083  |  0.773327 |  0.774933 | 0.778814 | 0.780776 |
	| CFFL          |  0.788805 |  0.790589 |  0.796744 |  0.794826 | 0.789652 | 0.792373 |
	| CFFL pretrain |  0.780196 |  0.78595  |  0.783675 |  0.787957 | 0.786441 | 0.78934  |

## Settings for experiment 2:
	- lr = 0.001
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'mean' ->fedavg
	- no lr decay
	- NOTE: To show that fedavg alone helps with both training stability -> acc and good fairness
	- folder name: Experiments_2020-05-01-22:44


### Results
	|         |   Distriubted |      CFFL |   CFFL pretrain |
	|:--------|--------------:|----------:|----------------:|
	| P10_0.1 |     0.17599   |  0.118817 |        0.606401 |
	| P10_1.0 |    -0.08173   |  0.171829 |        0.639351 |
	| P20_0.1 |     0.424413  |  0.631835 |        0.802583 |
	| P20_1.0 |     0.425838  |  0.430256 |        0.738453 |
	| P5_0.1  |    -0.139916  | -0.156414 |        0.686485 |
	| P5_1.0  |     0.0647306 | -0.35913  |        0.632759 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.806557 |  0.805308 |  0.794068 |  0.796476 | 0.803479 | 0.805754 |
	| Standalone    |  0.762801 |  0.75504  |  0.774175 |  0.781713 | 0.771097 | 0.768332 |
	| CFFL          |  0.793176 |  0.789161 |  0.789607 |  0.792061 | 0.796655 | 0.797547 |
	| CFFL pretrain |  0.788359 |  0.785192 |  0.792239 |  0.792596 | 0.788938 | 0.797413 |
_Note_: using 'fedavg' without lr decay, the fairness is ok, and the acc is good, too. Especially, using fedavg can help achieve higher fairness with more parties in the collaboration, as by P20 in the table.



## Settings for experiment 3:
	- lr = 0.1
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- n_freerider = 0
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'mean' ->fedavg
	- lr decay gamma: 0.955
	- NOTE: To study if fedavg and decay (combined) can produce better fairness and acc
	- folder name: Experiments_2020-05-01-19:15


### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |     0.226601  | 0.713442 |        0.905525 |
	| P10_1.0 |     0.143061  | 0.710929 |        0.912906 |
	| P20_0.1 |     0.081461  | 0.742324 |        0.903794 |
	| P20_1.0 |     0.0851699 | 0.718001 |        0.891763 |
	| P5_0.1  |     0.0885842 | 0.268299 |        0.853697 |
	| P5_1.0  |     0.155198  | 0.339107 |        0.903972 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.807449 |  0.80901  |  0.800714 |  0.804906 | 0.805709 | 0.807181 |
	| Standalone    |  0.762979 |  0.762756 |  0.77074  |  0.768778 | 0.780285 | 0.779081 |
	| CFFL          |  0.788136 |  0.786574 |  0.789251 |  0.790455 | 0.794157 | 0.794603 |
	| CFFL pretrain |  0.772926 |  0.773104 |  0.786887 |  0.781624 | 0.793533 | 0.793131 |
_Notes_ : lr decay or cffl seems to work relatively well when P is smaller (P=5, P=10), fedavg works relatively well when P is larger (P=20). Combining both seems to have a balancing effect on the fairness of settings with different parties in the collaboration. But it does not seems to have a significant impact on acc.


## Settings for experiment 4:
	- lr = 0.1
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- n_freerider = 1
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'sum'
	- lr decay gamma: 0.955
	- theta [0.1, 0.2, 0.3, ..., 1]
	- NOTE: To study that how thetas affect the ability to detect the free rider
	- folder name: Experiments_2020-05-01-16:53


### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |    0.0252604  | 0.990067 |       0.304686  |
	| P10_0.2 |    0.276762   | 0.99194  |       0.0425587 |
	| P10_0.3 |   -0.0972408  | 0.990834 |      -0.0161277 |
	| P10_0.4 |    0.0652672  | 0.987376 |      -0.130483  |
	| P10_0.5 |    0.197088   | 0.990618 |       0.196873  |
	| P10_0.6 |    0.152649   | 0.989459 |      -0.0419715 |
	| P10_0.7 |    0.142747   | 0.989183 |       0.0583182 |
	| P10_0.8 |    0.0816231  | 0.991773 |      -0.0556744 |
	| P10_0.9 |    0.0774481  | 0.981235 |       0.0499538 |
	| P10_1.0 |    0.18162    | 0.985714 |      -0.0955923 |
	| P20_0.1 |    0.112733   | 0.969994 |       0.352379  |
	| P20_0.2 |    0.212299   | 0.982845 |       0.442311  |
	| P20_0.3 |    0.117909   | 0.971231 |       0.427131  |
	| P20_0.4 |    0.158393   | 0.973984 |       0.393075  |
	| P20_0.5 |    0.111591   | 0.983125 |       0.454891  |
	| P20_0.6 |    0.0397978  | 0.97799  |       0.54212   |
	| P20_0.7 |    0.199981   | 0.967534 |       0.433822  |
	| P20_0.8 |    0.0967998  | 0.974352 |       0.683744  |
	| P20_0.9 |    0.089928   | 0.982608 |       0.529446  |
	| P20_1.0 |    0.0845959  | 0.968655 |       0.31476   |
	| P5_0.1  |   -0.121393   | 0.987688 |       0.700228  |
	| P5_0.2  |    0.0159597  | 0.99192  |       0.602733  |
	| P5_0.3  |    0.366193   | 0.99265  |       0.64757   |
	| P5_0.4  |    0.0778365  | 0.992066 |       0.586249  |
	| P5_0.5  |    0.0843404  | 0.994212 |       0.429371  |
	| P5_0.6  |    0.111163   | 0.989989 |       0.505372  |
	| P5_0.7  |   -0.144221   | 0.989677 |       0.454977  |
	| P5_0.8  |    0.189388   | 0.990468 |       0.508452  |
	| P5_0.9  |   -0.110076   | 0.993774 |       0.632428  |
	| P5_1.0  |   -0.00741947 | 0.992253 |       0.372125  |
	|               |   P10_0.1 |   P10_0.2 |   P10_0.3 |   P10_0.4 |   P10_0.5 |   P10_0.6 |   P10_0.7 |   P10_0.8 |   P10_0.9 |   P10_1.0 |   P20_0.1 |   P20_0.2 |   P20_0.3 |   P20_0.4 |   P20_0.5 |   P20_0.6 |   P20_0.7 |   P20_0.8 |   P20_0.9 |   P20_1.0 |   P5_0.1 |   P5_0.2 |   P5_0.3 |   P5_0.4 |   P5_0.5 |   P5_0.6 |   P5_0.7 |   P5_0.8 |   P5_0.9 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|---------:|
	| Distributed   |  0.805888 |  0.806646 |  0.809233 |  0.808653 |  0.808073 |  0.809946 |  0.807672 |  0.807761 |  0.809144 |  0.806869 |  0.81124  |  0.812846 |  0.809322 |  0.814987 |  0.811195 |  0.802141 |  0.815789 |  0.811151 |  0.805263 |  0.80504  | 0.807627 | 0.806378 | 0.80727  | 0.805352 | 0.805085 | 0.804728 | 0.804906 | 0.806869 | 0.804326 | 0.80785  |
	| Standalone    |  0.771409 |  0.770607 |  0.77025  |  0.771855 |  0.769402 |  0.768198 |  0.768064 |  0.76686  |  0.76463  |  0.769045 |  0.774621 |  0.768198 |  0.770116 |  0.771409 |  0.773149 |  0.770294 |  0.769135 |  0.769848 |  0.763782 |  0.771365 | 0.779393 | 0.781445 | 0.78149  | 0.777877 | 0.782337 | 0.780999 | 0.779037 | 0.776583 | 0.775022 | 0.778769 |
	| CFFL          |  0.77752  |  0.777342 |  0.781267 |  0.775736 |  0.773238 |  0.778368 |  0.784969 |  0.776628 |  0.778858 |  0.77694  |  0.78537  |  0.781579 |  0.783898 |  0.782293 |  0.784255 |  0.78198  |  0.78198  |  0.783675 |  0.784032 |  0.783051 | 0.770071 | 0.765477 | 0.770205 | 0.765388 | 0.762444 | 0.762266 | 0.766637 | 0.763827 | 0.765433 | 0.766771 |
	| CFFL pretrain |  0.779929 |  0.779661 |  0.782917 |  0.783095 |  0.783541 |  0.783541 |  0.785192 |  0.782649 |  0.782917 |  0.783541 |  0.785192 |  0.786396 |  0.786485 |  0.780731 |  0.789295 |  0.786887 |  0.781267 |  0.787913 |  0.78149  |  0.780999 | 0.782426 | 0.78876  | 0.788715 | 0.788046 | 0.787779 | 0.788403 | 0.786485 | 0.786887 | 0.785504 | 0.786574 |
_Notes_ : Various thetas __cannot__ effectively take out free rider. Furthermore, because in the end the free rider is still __in__ the reliable set, the fairness measure is _not_ an accurate reflection of true fairness because it includes the free rider.


## Settings for experiment 5:
	- lr = 0.001
	- grad_clip = 0.01
	- B = 16
	- sample_size_cap = [5, 2000], [10, 4000], [20, 8000]
	- alpha = 5
	- n_freerider = 1
	- MLP (64 hidden neurons) with NLLLoss()
	- aggregate mode: 'mean' -> fedavg
	- no lr decay
	- NOTE: To study if fedavg alone can affect the ability to detect the free rider
	- folder name: Experiments_2020-05-02-07:02
### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |     0.0752315 | 0.988631 |        0.635469 |
	| P10_1.0 |     0.151269  | 0.982865 |        0.258348 |
	| P20_0.1 |     0.164302  | 0.983251 |        0.54308  |
	| P20_1.0 |     0.0722481 | 0.982241 |        0.391062 |
	| P5_0.1  |    -0.117507  | 0.994388 |        0.842333 |
	| P5_1.0  |     0.0779394 | 0.983099 |        0.569457 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.809233 |  0.806913 |  0.798573 |  0.801249 | 0.804728 | 0.806423 |
	| Standalone    |  0.763872 |  0.73479  |  0.766949 |  0.767038 | 0.778591 | 0.782114 |
	| CFFL          |  0.763292 |  0.768599 |  0.769447 |  0.772748 | 0.775067 | 0.775781 |
	| CFFL pretrain |  0.757939 |  0.743265 |  0.767306 |  0.764987 | 0.766949 | 0.768287 |
_Notes_ : using 'fedavg' only without decay still __cannot__ effectively take out free rider.





## Credit Sum experiments

## Settings for experiment 6:
	- lr = 0.1
	- alpha = 5
	- n_freerider = 0
	- aggregate mode: 'credit-sum' -> credit-weighted sum
	- lr decay gamma = 0.955
	- NOTE: To study if credit-sum and lr decay can give good performance
	- folder name: Experiments_2020-05-02-11:22

### Results
	|         |   Distriubted |     CFFL |   CFFL pretrain |
	|:--------|--------------:|---------:|----------------:|
	| P10_0.1 |    -0.0463232 | 0.82931  |        0.898302 |
	| P10_1.0 |     0.14933   | 0.849454 |        0.926375 |
	| P20_0.1 |     0.0703618 | 0.803697 |        0.899261 |
	| P20_1.0 |     0.0931343 | 0.806263 |        0.925249 |
	| P5_0.1  |     0.0985428 | 0.887553 |        0.957737 |
	| P5_1.0  |     0.434131  | 0.882981 |        0.955816 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.807315 |  0.808742 |  0.803613 |  0.79884  | 0.809054 | 0.804773 |
	| Standalone    |  0.763604 |  0.770339 |  0.76686  |  0.76851  | 0.780508 | 0.779126 |
	| CFFL          |  0.785236 |  0.786441 |  0.788626 |  0.791035 | 0.79496  | 0.795986 |
	| CFFL pretrain |  0.768912 |  0.781401 |  0.780687 |  0.782337 | 0.793086 | 0.791436 |


## Settings for experiment 7:
		- lr = 0.1
		- n_freerider = 1
		- MLP (64 hidden neurons) with NLLLoss()
		- aggregate mode: 'credit-sum' -> credit-weighted sum
		- no lr decay
		- NOTE: To study if credit-sum alone can detect free riders
		- folder name: Experiments_2020-05-02-13:31

### Results

	|         |   Distriubted |       CFFL |   CFFL pretrain |
	|:--------|--------------:|-----------:|----------------:|
	| P10_0.1 |    -0.206762  |   0.989472 |        0.639298 |
	| P10_1.0 |     0.100799  |   0.9861   |        0.371859 |
	| P20_0.1 |     0.193636  |   0.985051 |        0.669845 |
	| P20_1.0 |     0.0308375 |   0.983926 |        0.645877 |
	| P5_0.1  |     0.197668  |   0.99303  |        0.835889 |
	| P5_1.0  |    -0.0267162 | nan        |      nan        |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.809233 |  0.807359 |  0.791882 |  0.801739 | 0.807493 | 0.80562  |
	| Standalone    |  0.762846 |  0.763961 |  0.768555 |  0.775022 | 0.777342 | 0.780865 |
	| CFFL          |  0.764273 |  0.765611 |  0.772346 |  0.771855 | 0.771543 | 0.66405  |
	| CFFL pretrain |  0.758252 |  0.762578 |  0.764228 |  0.76744  | 0.769715 | 0.554193 |



## Settings for experiment 8:
		- lr = 0.1
		- n_freerider = 1
		- aggregate mode: 'credit-sum' -> credit-weighted sum
		- lr decay gamma = 0.955
		- NOTE: To study if lr decay and credit-sum can detect free riders good performance
		- folder name: Experiments_2020-05-02-13:31

### Results

	|         |   Distriubted |       CFFL |   CFFL pretrain |
	|:--------|--------------:|-----------:|----------------:|
	| P10_0.1 |     0.0263052 |   0.990256 |        0.988879 |
	| P10_1.0 |     0.31689   |   0.98461  |        0.975409 |
	| P20_0.1 |     0.0940637 |   0.990699 |        0.985068 |
	| P20_1.0 |     0.0985547 |   0.984925 |        0.986913 |
	| P5_0.1  |    -0.031606  |   0.996185 |        0.991604 |
	| P5_1.0  |     0.0849204 | nan        |        0.987257 |
	|               |   P10_0.1 |   P10_1.0 |   P20_0.1 |   P20_1.0 |   P5_0.1 |   P5_1.0 |
	|:--------------|----------:|----------:|----------:|----------:|---------:|---------:|
	| Distributed   |  0.809099 |  0.808207 |  0.799777 |  0.798216 | 0.807895 | 0.804505 |
	| Standalone    |  0.767351 |  0.768644 |  0.770919 |  0.771231 | 0.780464 | 0.776896 |
	| CFFL          |  0.780687 |  0.785281 |  0.78876  |  0.790009 | 0.7843   | 0.673283 |
	| CFFL pretrain |  0.778412 |  0.780776 |  0.781713 |  0.782739 | 0.791392 | 0.790589 |